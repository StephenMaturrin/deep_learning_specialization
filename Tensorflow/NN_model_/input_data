import numpy as np
import pandas as pd
from tensorflow.python.data import Dataset
from Project import constants as c
import random
import tensorflow as tf


############################## Description ##############################
# Setting up input data.
#
###################################################################





def my_input_fn(features, targets, batch_size, shuffle = False, num_epochs=None):
    """Trains a linear regression model of one feature.

    Args:
      features: pandas DataFrame of features
      targets: pandas DataFrame of targets
      batch_size: Size of batches to be passed to the model
      shuffle: True or False. Whether to sfehuffle the data.
      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely
    Returns:
      Tuple of (features, labels) for next data batch

      Slices

    The function starts by using the tf.data.Dataset.from_tensor_slices function to create a tf.data.Dataset
    representing slices of the array. The array is sliced across the first dimension. For example, an array
    containing the mnist training data has a shape of (60000, 28, 28). Passing this to from_tensor_slices
    returns a Dataset object containing 60000 slices, each one a 28x28 image.

    Manipulation

    Currently the Dataset would iterate over the data once, in a fixed order, and only produce a single
    element at a time. It needs further processing before it can be used for training. Fortunately,
    the tf.data.Dataset class provides methods to better prepare the data for training. The next line of the
    input function takes advantage of several of these methods:

    -->The shuffle method uses a fixed-size buffer to shuffle the items as they pass through. In this case the
     buffer_size is greater than the number of examples in the Dataset, ensuring that the data is completely
     shuffled (The Iris data set only contains 150 examples).

    -->The repeat method restarts the Dataset when it reaches the end. To limit the number of epochs, set the
    count argument.

    -->The batch method collects a number of examples and stacks them, to create batches. This adds a dimension
    to their shape. The new dimension is added as the first dimension. The following code uses the batch method
    on the MNIST Dataset, from earlier. This results in a Dataset containing 3D arrays representing stacks of
    (28,28) images:

    Return

    At this point the Dataset contains (features_dict, labels) pairs.
    This is the format expected by the train and evaluate methods,
    so the input_fn returns the dataset.
    The labels can/should be omitted when using the predict method.

    https://www.tensorflow.org/get_started/datasets_quickstart#passing_input_fn_data_to_your_model
    """

    features = {key: np.array(value) for key, value in dict(features).items()}

    # Construct a dataset, and configure batching/re peating
    ds = Dataset.from_tensor_slices((features, targets))  # warning: 2GB limit
    # Shuffle the data, if specified
    if shuffle:
        ds = ds.shuffle(10000)
    ds = ds.batch(batch_size).repeat(num_epochs)

    # Return the next batch of data
    features, labels = ds.make_one_shot_iterator().get_next()
    return features, labels
    # return ds.make_one_shot_iterator()

def my_input_fn_v2(features, targets, batch_size, shuffle = False, num_epochs=None):
    """Trains a linear regression model of one feature.
    """

    # features = {key: np.array(value) for key, value in dict(features).items()}

    # Construct a dataset, and configure batching/re peating

    # print("SHAPE",targets.shape)
    targets = np.reshape(targets,(targets.shape[0],1))
    # print("SHAPE",targets.shape)
    ds = Dataset.from_tensor_slices((features, targets))  # warning: 2GB limit
    # Shuffle the data, if specified
    if shuffle:
        ds = ds.shuffle(10000)
    ds = ds.batch(batch_size).repeat(num_epochs)

    # Return the next batch of data
    features, labels = ds.make_one_shot_iterator().get_next()

    # labels= tf.reshape(labels,shape=(batch_size,1))
    return features, labels
    # return ds.make_one_shot_iterator()


def create_datasets(dataset, number_of_die_to_validate):
    """Select number_of_die_to_validate from de dataset to generate
        a dataset for the validation. It's selected randomly.

        Args:
          dataset: pandas DataFrame
          number_of_die_to_validate : A certain number of Dies

        Returns:
          Dataframe for the testing without the validation data
          Dataframe selected by the Dies for the validation test

        """

    dies_selected = []
    frames = []
    frames_training = []
    # Taking a number_of_die_to_validate while it checks if it was already taken

    for i in range(number_of_die_to_validate):

        # Check if the die was already selected, if not can continue if yes while (no) select another die
        while True:
            die = random.choice(c.DIES)
            if die not in dies_selected:
                break
        dies_selected.append(die)
        frames.append(dataset.loc[dataset['Die'] == int(die)])
        # training_dataset= (dataset.loc[dataset['Die'] == int(die)] for die not in)
    for die in c.DIES:
        if die not in dies_selected:
            frames_training.append(dataset.loc[dataset['Die'] == int(die)])


    # Return the training dataset and the  dataframe for the validation
    return dies_selected,pd.concat(frames_training), pd.concat(frames)

