import tensorflow as tf
import numpy as np
from datetime import datetime
import pandas as pd
import os
from ms255613_project.generic_functions import  get_logdir
import os
from os import listdir
from os.path import isfile, join
from matplotlib import pyplot as plt
from tensorflow.contrib.layers import batch_norm

__author__ = "Miguel Solinas Jr."
__credits__ = "Miguel Solinas Jr."
__version__ = "0.1.1"
__maintainer__ = "Miguel Solinas Jr."
__email__ = "migue.solinas@gmail.com"
__status__ = "Project"

############################## Description ##############################
# This object implements not only the train and test step but also the the computational graph and the tensorboard.
#
###################################################################

class NN_model():

    #
    # keep_prob = tf.placeholder(tf.float64)

    def __init__(self, dataset, num_epochs, batch_size, model,learning_rate,board_metrics, n_layer,activation_fn,metrics,optimizer, dropout, logdir= "/home/tmpext4/ms255613/ANN/log"):
            self.dataset= dataset
            self.X = tf.placeholder(tf.float64, shape=(None, n_layer[0]), name='X')
            self.n_layer=n_layer
            self.model = model
            self.logdir = get_logdir(logdir)
            self.learning_rate = learning_rate
            self.steps = num_epochs
            self.batch_size= batch_size
            self.metrics= metrics
            self.activation_fn=activation_fn
            self.sess = None
            self.k_p_dropout = dropout
            self.keep_prob = tf.placeholder(tf.float64)
            self.optimizer = optimizer
            self.board_metrics = board_metrics

            print (self.logdir)
            if not os.path.exists(self.logdir):
                os.mkdir(self.logdir)
            # mdir = self.logdir
            # print (mdir)

            self.f = open(os.path.abspath(self.logdir) + "/model.txt", "a+")
            # self.f = open(os.path.dirname(self.logdir)+ "model.txt", "w+")
            self.f.write("model : " + self.model+ "\n")
            self.f.write("n_layer : " + str(self.n_layer) +"\n")
            self.f.write("learning_rate : " + str(self.learning_rate)+"\n")
            self.f.write("num_epochs : " + str(self.steps)+"\n")
            self.f.write("batch_size : " + str(self.batch_size)+"\n")




            if self.model =="Lin_Reg":
                self.Y = tf.placeholder(tf.float64, shape=(None), name='Y')
            else:
                self.Y = tf.placeholder(tf.int64, shape=(None), name='Y')


            #Batch normalization






    def variables_summaries(self,name,var):
        with tf.name_scope("summaries/"+name):
            mean = tf.reduce_mean(var)
            tf.summary.scalar("mean",mean)
            with tf.name_scope("stddev"):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))  # stddev is used to quantify the amount of variation or dispersion of a set of data values
                                                                       # A low standard deviation indicates that the data points tend to be close to the mean
                                                                       # (also called the expected value) of the set, while a high standard deviation indicates
                                                                       # that the data points are spread out over a wider range of values.
            tf.summary.scalar("stddv_"+name,stddev)
            tf.summary.scalar("max_"+name,tf.reduce_max(var))
            tf.summary.scalar("min_"+name,tf.reduce_min(var))
            tf.summary.histogram("h_"+name,var)



    def nn_layer(self,input_tensor, input_dim, output_dim, layer_name, activation_fn= tf.nn.relu):

        #iplement tf.nn.batch_normalization

        assert input_tensor.shape[1]==input_dim, "Check dimensions   %d != input_dim %d" % (input_tensor.shape[1],input_dim)

        with tf.name_scope(layer_name):
            with tf.name_scope("weights"):

                weight = tf.get_variable("W_"+layer_name, shape=(input_dim,output_dim), initializer= tf.glorot_uniform_initializer(),dtype=tf.float64)
                self.variables_summaries("Weight",weight)
            with tf.name_scope("bias"):
                bias = tf.Variable(tf.zeros(shape=(1,output_dim)),name="bias")
                bias = tf.cast(bias,tf.float64)
                self.variables_summaries("Bias_"+layer_name,bias)
            with tf.name_scope("pre_activation"):
                pre_activation = tf.add(tf.matmul(input_tensor,weight),bias)


                tf.summary.histogram("pre_activation",pre_activation)
            with tf.name_scope("activation"):
                activation= activation_fn(pre_activation)
                tf.summary.histogram("activation", activation)

            with tf.name_scope("dropout_activation"):
                drop_out = tf.nn.dropout(activation, self.keep_prob)  # DROP-OUT here P = 1 -kp
                # activation = activation_fn(pre_activation)
                tf.summary.histogram("drop_out", drop_out)

        return weight, bias, drop_out


    

    def loss_and_train (self, hypothesis,weights):

        if (self.model == "Log_Reg"):
            with tf.name_scope('cross_entropy'):
                cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=self.Y, logits=hypothesis)
                tf.summary.scalar("cross_entropy", cross_entropy)
            with tf.name_scope("train"):
                train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cross_entropy)
                self.variables_summaries("loss", cross_entropy)
                return train_step
        if (self.model == "Lin_Reg"):
            with tf.name_scope("MSE"):
                # loss = tf.losses.mean_squared_error(labels=self.Y, predictions=hypothesis)
                # aux = (hypothesis-self.Y)
                loss = tf.reduce_mean(tf.square(hypothesis-self.Y))
                # reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
                # loss_l1 = tf.add_n([loss] + reg_losses, name="loss")
                tf.summary.scalar("loss", loss)
            with tf.name_scope("train"):

                reg_aux= 0
                for w in weights:
                    reg_aux = reg_aux +  tf.nn.l2_loss(w)



                loss_l1 = tf.reduce_mean(loss + reg_aux * 0.9)

                train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.899).minimize(loss_l1)

                # train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.899).minimize(loss_l1)

                # train_step = tf.train.FtrlOptimizer(learning_rate=self.learning_rate, l1_regularization_strength=1,l2_regularization_strength=0.5).minimize(loss_l1)
                tf.summary.scalar("loss",loss_l1)
                self.variables_summaries("loss",loss_l1)
                return train_step,loss_l1,self.Y,hypothesis


    def accuracy(self,hypothesis):
        with tf.name_scope("accuracy"):
            with tf.name_scope("correct_prediction"):

                if (self.model == "Log_Reg"):

                    correct_prediction = tf.equal(tf.argmax(hypothesis,1),self.Y)
                    accuracy =  tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
                    tf.summary.scalar("accuracy", accuracy)
                    return accuracy

                if (self.model == "Lin_Reg"):

                    correct_prediction = tf.subtract(self.Y,hypothesis)
                    accuracy = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction,self.Y)),100) # Error prediction in %
                    tf.summary.scalar("accuracy", accuracy)
                    return accuracy




    def feed_dict(self, train):

        if train:
            xs, ys = self.dataset.train.next_batch(1000)
            k = 1
        else:
            xs, ys = self.dataset.test.images, self.dataset.test.labels
            k = 1

        print( xs, ys)
        return {self.X: xs, self.Y: ys, self.keep_prob: k}

    def feed_dict_v2(self,train_iterator, train,all_test=False):


        if train:
            xs, ys = self.sess.run(train_iterator)

            k = self.k_p_dropout

        elif(not all_test):
            xs, ys = self.sess.run(self.dataset.get_test_dataset(num_epochs=self.steps))
            k = 1

        else:
            xs, ys = self.sess.run(self.dataset.get_all_dataset())
            k = 1

        return {self.X: xs, self.Y: ys, self.keep_prob: k}





    def train(self):

        self.sess = tf.InteractiveSession()
        sess = self.sess
        train_iterator = self.dataset.get_train_dataset(batch_size=self.batch_size, num_epochs=self.steps)

        # for i in range(10):
        #     print(sess.run(train_iterator))

        # train_iterator_handle = sess.run(train_iterator.string_handle())

        test_iterator = self.dataset.get_test_dataset(num_epochs=self.steps)
        # test_iterator_handle = sess.run(test_iterator.string_handle())



        layers = {}                # activation_layer_1 = self.nn_layer(self.X, self.n_layer[0], self.n_layer[1], "layer_1", activation_fn= tf.nn.relu)
                                   # hypothesis = self.nn_layer(activation_layer_1, self.n_layer[1], self.n_layer[2], "layer_2", activation_fn= tf.identity)
        weights = []
        for index,l_ in enumerate(self.n_layer):

            if(index == len(self.n_layer)-2):
                weight, bias, activation = self.nn_layer(layers[index]["activation"], self.n_layer[index], self.n_layer[index+1], "layer_"+str(index+1), activation_fn=tf.identity)
                weights.append(weight)
                layers.update({index+1 : {"weight":weight, "bias":bias ,"activation" : activation }})
                break
            if (index == 0):
                 weight, bias, activation = self.nn_layer(self.X, self.n_layer[index], self.n_layer[index+1], "layer_"+str(index+1),activation_fn=self.activation_fn)
                 weights.append(weight)
                 layers.update({index+1 : {"weight": weight, "bias": bias, "activation" : activation }})
            else :
                weight, bias, activation = self.nn_layer(layers[index]["activation"], self.n_layer[index], self.n_layer[index + 1],"layer_" + str(index + 1), activation_fn=self.activation_fn)
                weights.append(weight)
                layers.update({index + 1: {"weight": weight, "bias": bias, "activation" : activation}})



        _hypothesis = layers[len(self.n_layer)-1]["activation"]
        _accuracy=self.accuracy(_hypothesis)
        # rain_step, cost, self.Y, hypothesis
        _train_step,_loss, _Y, ___hypothesis = self.loss_and_train(_hypothesis,weights)
        tf.global_variables_initializer().run()

        merged = tf.summary.merge_all()
        train_writer = tf.summary.FileWriter(self.logdir + '/train', sess.graph)
        test_writer = tf.summary.FileWriter(self.logdir + '/test', sess.graph)



        for i in range(self.steps):
            if i %  1000 ==0:

                # summary, acc = sess.run([merged,_accuracy],feed_dict=self.feed_dict(False))
                # train_writer.add_summary(summary, i)
                summary, acc,__hypo = sess.run([merged,_accuracy,_hypothesis], feed_dict=self.feed_dict_v2(test_iterator, False))
                if self. board_metrics: test_writer.add_summary(summary,i)

                print('num_epochs n°',i, 'accuracy', acc)
                # if i %  20 ==0:
                #     print("Y",__Y.shape,"HYPO",__hypo.shape)
            elif (self.board_metrics and  i% 10001==0):
                # if i% 10000==0:
                run_options= tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)
                run_metadata = tf.RunMetadata()
                # summary, _,__loss = sess.run([merged,_train_step,_loss], feed_dict=self.feed_dict(True), options= run_options, run_metadata= run_metadata)
                summary, _, __loss, ___Y,____hypothesis = sess.run([merged, _train_step, _loss, _Y, ___hypothesis], feed_dict=self.feed_dict_v2(train_iterator,True),options=run_options, run_metadata=run_metadata)
                # print(___Y.shape, ____hypothesis.shape)
                # _ = self.sess.run([__train_ste], feed_dict=self.feed_dict(False),options = run_options, run_metadata = run_metadata)
                train_writer.add_run_metadata(run_metadata,'num_epochs_%04d' % i )
                train_writer.add_summary(summary,i)
                print("num_epochs n°:", i, "  LOSS:", np.sqrt(__loss))
            else:
                    # for i in range(250):
                summary, _, __loss = sess.run([merged, _train_step, _loss],feed_dict=self.feed_dict_v2(train_iterator, True))
                if self.board_metrics: train_writer.add_summary(summary, i)

                # _, __loss= sess.run([_train_step, _loss],feed_dict=self.feed_dict_v2(train_iterator, True))
                if i % 999== 0:
                        print("num_epochs n°:", i, "  LOSS:", np.sqrt(__loss))

                    # summary, _ = sess.run([merged, _train_step, ], feed_dict=self.feed_dict( True))
                    #

        self.f.write("Loss :" + str(np.sqrt(__loss))+"\n")
        self.f.write("Accuracy :" + str(acc)+ "\n")
        self.f.close()

        outdir = self.logdir + 'Weights_Bias'
        if not os.path.exists(outdir):
            os.mkdir(outdir)



        for index, value in enumerate(self.n_layer):
            if index == len(self.n_layer)-1:
                break
            weight = layers[index+1]["weight"]
            bias = layers[index+1]["bias"]
            __weight,__bias = sess.run([weight,bias],feed_dict=self.feed_dict_v2(test_iterator,False))
            # __weight, __bias = sess.run([weight, bias], feed_dict=self.feed_dict( False))
            w = pd.DataFrame(data=__weight.astype(float))
            w.to_csv(os.path.join(outdir, 'w'+str(index+1)+'.csv'), sep=';', header=False, float_format='%.4f', index=False)
            b = pd.DataFrame(data=__bias.astype(float))
            b.to_csv(os.path.join(outdir, 'b'+str(index+1)+'.csv'), sep=';', header=False, float_format='%.4f', index=False)



        train_writer.close()
        test_writer.close()

        print("tensorboard --logdir=%s --port=  " % self.logdir)

    def test(self,dataset,path): # use W and B from train

        self.sess = tf.InteractiveSession()
        sess =self.sess
        self.dataset = dataset

        layers = os.listdir(path)


        activations = []
        for i in range(int(len(layers) / 2)):  # n° of layers = (#B+#W)/2
            b = pd.read_csv(path + "/b" + str(i + 1) + ".csv", sep=";", header=None)
            w = pd.read_csv(path + "/w" + str(i + 1) + ".csv", sep=";", header=None)

            w_tf = tf.Variable(name="w" + str(i + 1), initial_value=w)
            b_tf = tf.Variable(name="b" + str(i + 1), initial_value=b)
            if i == 0:
                pre_activation = tf.add(tf.matmul(self.X, w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))
            elif i == len(layers) - 1:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(tf.identity(pre_activation))
            else:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))

        all_iterator = self.dataset.get_all_dataset()


        tf.global_variables_initializer().run()


        if (self.model == "Lin_Reg"):

            correct_prediction  = tf.subtract(self.Y, activations[-1])
            accur = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction, self.Y)), 100)  # Error prediction in %

            _acc, _Y,_h = sess.run([accur,self.Y,activations[-1]], self.feed_dict_v2(all_iterator, False,True))

            major_ticks = np.arange(min(_Y), max(_Y), 50)
            minor_ticks = np.arange((min(_Y)), max(_Y), 150)
            plt.xticks(minor_ticks)
            plt.yticks(major_ticks)
            plt.grid(which='minor', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)
            plt.grid(which='major', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)

            plt.title(' Fmax  Prediction Vs Measured ')
            plt.xlabel('Fmax  Measured')
            plt.ylabel('Fmax Predicted')

            plt.scatter(_Y, _h, marker='^')

            plt.plot([_h.min(), _h.max()],
                     [_h.min(), _h.max()], 'k--', lw=3)
            plt.legend()
            # And a corresponding grid

            # Or if you want different settings for the grids:
            plt.grid(which='minor', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)
            plt.grid(which='major', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)
            plt.show()
            print('mean accuracy = ',100-abs(_acc))


        if( self.model == "Log_Reg"):
            prediction = tf.equal(self.Y, tf.argmax(activations[-1], 1))
            prediction_cast = tf.cast(prediction, tf.float32)
            accur = tf.reduce_mean(prediction_cast)

            print(sess.run([accur], self.feed_dict_v2(all_iterator, False,True)))

    def predict(self, path,input_fn):



        self.sess = tf.InteractiveSession()
        sess =self.sess
        self.dataset = input_fn

        layers = os.listdir(path)


        activations = []
        for i in range(int(len(layers) / 2)):  # n° of layers = (#B+#W)/2
            b = pd.read_csv(path + "/b" + str(i + 1) + ".csv", sep=";", header=None)
            w = pd.read_csv(path + "/w" + str(i + 1) + ".csv", sep=";", header=None)

            # if i ==2:
            #Prunning
            # print("W : ", i + 1, "  ", w[np.abs(w) < 0.005].count().sum())
            # w[np.abs(w) < 0.005] = 0


            w_tf = tf.Variable(name="w" + str(i + 1), initial_value=w)
            b_tf = tf.Variable(name="b" + str(i + 1), initial_value=b)
            if i == 0:
                pre_activation = tf.add(tf.matmul(self.X, w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))
            elif i == len(layers) - 1:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(tf.identity(pre_activation))
            else:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))

        # all_iterator = self.dataset.get_all_dataset()


        tf.global_variables_initializer().run()


        if (self.model == "Lin_Reg"):

            correct_prediction  = tf.subtract(self.Y, activations[-1])
            accur = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction, self.Y)), 100)  # Error prediction in %
            X, Y = sess.run(self.dataset)
            _acc, _Y,_h = sess.run([accur,self.Y,activations[-1]],{self.X : X , self.Y: Y, self.keep_prob: 1})
            print("accuracy", _acc)
            return _h


        if( self.model == "Log_Reg"):
            prediction = tf.equal(self.Y, tf.argmax(activations[-1], 1))
            prediction_cast = tf.cast(prediction, tf.float32)
            accur = tf.reduce_mean(prediction_cast)

            return prediction



