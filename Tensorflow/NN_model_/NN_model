import tensorflow as tf
import numpy as np
from datetime import datetime
import pandas as pd
import os
from tensorflow.python.framework import ops
import operator
import keras
from ms255613_project import features_manager as fm
from ms255613_project.generic_functions import  get_logdir
import os
from os import listdir
from os.path import isfile, join
from matplotlib import pyplot as plt
from tensorflow.contrib.layers import batch_norm
from ms255613_project import constants  as c
from ms255613_project import data_visualization as dv
from ms255613_project import normalize_functions as nf
from ms255613_project import input_builder as ib
import sklearn.metrics as metrics

from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error

__author__ = "Miguel Solinas Jr."
__credits__ = "Miguel Solinas Jr."
__version__ = "0.1.1"
__maintainer__ = "Miguel Solinas Jr."
__email__ = "migue.solinas@gmail.com"
__status__ = "Project"

############################## Description ##############################
# This object implements not only the train and test step but also the the computational graph and the tensorboard.
#
###################################################################

class NN_model():

    #
    # keep_prob = tf.placeholder(tf.float64)

    def __init__(self, dataset, num_epochs,batch_norm, batch_size, model,learning_rate,board_metrics, n_layer,activation_fn,optimizer, dropout=None,normalization=None, lambd_L1=None,lambd_L2=None, logdir= "/home/tmpext4/ms255613/ANN/log"):

            tf.reset_default_graph()
            self.dataset= dataset
            self.X = tf.placeholder(tf.float64, shape=(None, n_layer[0]), name='X')
            self.Y = tf.placeholder(tf.float64, shape=(None), name='Y')
            self.learning_rate_dec = tf.placeholder(tf.float32)
            self.is_training = tf.placeholder(tf.bool)
            self.n_layer=n_layer
            self.model = model
            self.logdir = get_logdir(logdir)
            self.learning_rate = learning_rate
            self.steps = num_epochs
            self.batch_size= batch_size
            self.activation_fn=activation_fn
            self.sess = None
            self.k_p_dropout = dropout
            self.keep_prob = tf.placeholder(tf.float64)
            self.optimizer = optimizer
            self.board_metrics = board_metrics
            self.normalization = normalization
            self.lambd_L1 = lambd_L1
            self.lambd_L2 = lambd_L2
            self.batch_norm =  batch_norm

            #
            # print (self.logdir)

            if board_metrics:
                if not os.path.exists(self.logdir):
                    os.mkdir(self.logdir)

                self.f = open(os.path.abspath(self.logdir) + "/model.txt", "a+")
                self.f.write("model : " + self.model+ "\n")
                self.f.write("n_layer : " + str(self.n_layer) +"\n")
                self.f.write("learning_rate : " + str(self.learning_rate)+"\n")
                self.f.write("num_epochs : " + str(self.steps)+"\n")
                self.f.write("batch_size : " + str(self.batch_size)+"\n")


            # if self.model =="Lin_Reg":
            #     self.Y = tf.placeholder(tf.float64, shape=(None), name='Y')
            # else:
            #     self.Y = tf.placeholder(tf.int64, shape=(None), name='Y')


            #Batch normalization


    def variables_summaries(self,name,var):
        with tf.name_scope("summaries/"+name):
            mean = tf.reduce_mean(var)
            tf.summary.scalar("mean",mean)
            with tf.name_scope("stddev"):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))  # stddev is used to quantify the amount of variation or dispersion of a set of data values
                                                                       # A low standard deviation indicates that the data points tend to be close to the mean
                                                                       # (also called the expected value) of the set, while a high standard deviation indicates
                                                                       # that the data points are spread out over a wider range of values.
            tf.summary.scalar("stddv_"+name,stddev)
            tf.summary.scalar("max_"+name,tf.reduce_max(var))
            tf.summary.scalar("min_"+name,tf.reduce_min(var))
            tf.summary.histogram("h_"+name,var)



    def nn_layer(self,input_tensor, input_dim, output_dim, layer_name, layer_BN, activation_fn= tf.nn.relu):

        #iplement tf.nn.batch_normalization

        # assert input_tensor.shape[1]==input_dim, "Check dimensions   %d != input_dim %d" % (input_tensor.shape[1],input_dim)

        with tf.name_scope(layer_name):
            with tf.name_scope("weights"):

                weight = tf.get_variable("W_"+layer_name, shape=(input_dim,output_dim), initializer= tf.glorot_uniform_initializer(),dtype=tf.float64)
                self.variables_summaries("Weight",weight)
            with tf.name_scope("bias"):
                bias = tf.Variable(tf.zeros(shape=(1,output_dim)),name="bias")
                bias = tf.cast(bias,tf.float64)
                self.variables_summaries("Bias_"+layer_name,bias)
            with tf.name_scope("pre_activation"):
                pre_activation = tf.add(tf.matmul(input_tensor,weight),bias)
                tf.summary.histogram("pre_activation",pre_activation)



            with tf.name_scope("activation"):
                activation_l= activation_fn(pre_activation)
                tf.summary.histogram("activation", activation_l)


            if (self.batch_norm and layer_BN):
                with tf.name_scope("batch_normalization_user"):
                    activation = tf.layers.batch_normalization(activation_l, training=self.is_training)
                    tf.summary.histogram("batch_norm", activation)
                    activation_l = activation


            else:
                activation_l= activation_l

        #
            with tf.name_scope("dropout_activation"):
                drop_out = tf.nn.dropout(activation_l, self.keep_prob)  # DROP-OUT here P = 1 -kp
                # activation = activation_fn(pre_activation)
                tf.summary.histogram("drop_out", drop_out)

        return weight, bias, drop_out#, self.keep_prob


    

    def loss_and_train (self, hypothesis,weights):

        if (self.model == "Log_Reg"):
            with tf.name_scope('cross_entropy'):
                cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=self.Y, logits=hypothesis)
                tf.summary.scalar("cross_entropy", cross_entropy)
            with tf.name_scope("train"):
                train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate_dec).minimize(cross_entropy)
                self.variables_summaries("loss", cross_entropy)
                return train_step
        if (self.model == "Lin_Reg"):
            with tf.name_scope("MSE"):
                # loss = tf.losses.mean_squared_error(labels=self.Y, predictions=hypothesis)
                # aux = (hypothesis-self.Y)
                loss = tf.reduce_mean(tf.square(hypothesis-self.Y))
                # reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
                # loss_l1 = tf.add_n([loss] + reg_losses, name="loss")
                tf.summary.scalar("loss", loss)
                # cost = tf.reduce_mean(tf.square(hypothesis - self.Y))
            with tf.name_scope("train"):

                # reg_aux= 0
                # for w in weights:
                #     # L2
                #     # reg_aux = reg_aux +  tf.nn.l2_loss(w)
                #     # L1
                #     reg_aux = reg_aux + tf.reduce_sum(w)
                #
                # #lambda = 0.9
                # loss_l1 = tf.reduce_mean(loss + (reg_aux/self.batch_size) * 0.9)
                if(self.normalization==None):
                    loss_R = loss


                #L1
                if(self.normalization== "L1"):
                    reg_aux = 0
                    for w in weights:
                        reg_aux = reg_aux + tf.reduce_sum(w)
                    loss_R = tf.reduce_mean(loss + (reg_aux / self.batch_size) *self.lambd_L1)

                #L2
                elif(self.normalization== "L2"):
                    reg_aux = 0
                    for w in weights:
                        reg_aux = reg_aux + tf.nn.l2_loss(w)
                    loss_R = tf.reduce_mean(loss + reg_aux * self.lambd_L2)

                with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
                    # https: // stackoverflow.com / questions / 43234667 / tf - layers - batch - normalization - large - test - error
                    train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999).minimize(loss_R)

                # train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.899).minimize(loss_l1)

                # train_step = tf.train.FtrlOptimizer(learning_rate=self.learning_rate, l1_regularization_strength=1,l2_regularization_strength=0.5).minimize(loss_l1)
                tf.summary.scalar("loss",loss_R)
                self.variables_summaries("loss",loss_R)
                return train_step,loss_R,self.Y,hypothesis


    def accuracy(self,hypothesis):
        with tf.name_scope("accuracy"):
            with tf.name_scope("correct_prediction"):

                if (self.model == "Log_Reg"):

                    correct_prediction = tf.equal(tf.argmax(hypothesis,1),self.Y)
                    accuracy =  tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
                    tf.summary.scalar("accuracy", accuracy)
                    return accuracy

                if (self.model == "Lin_Reg"):

                    correct_prediction = tf.subtract(self.Y,hypothesis)
                    accuracy = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction,self.Y)), 100) # Error prediction in %

                    # a = tf.reduce_mean(tf.equal(self.Y,hypothesis))
                    accuracy = 100 - tf.abs( accuracy)
                    accuracy= tf.reduce_mean( correct_prediction)
                    tf.summary.scalar("accuracy", accuracy)
                    return accuracy




    def feed_dict(self, train):

        if train:
            xs, ys = self.dataset.train.next_batch(1000)
            k = 1
        else:
            xs, ys = self.dataset.test.images, self.dataset.test.labels
            k = 1

        print( xs, ys)
        return {self.X: xs, self.Y: ys, self.keep_prob: k}

    def feed_dict_v2(self, train,features=c.FEATURES[3:len(c.FEATURES)], all_test=False  ): #c.FEATURES[2:len(c.FEATURES)]


        if train:
            xs, ys = self.sess.run(self.dataset.get_train_dataset(batch_size=self.batch_size, num_epochs=self.steps,features=features))

            k = self.k_p_dropout
            is_train = True
        elif(not all_test):
            xs, ys = self.sess.run(self.dataset.get_test_dataset(num_epochs=self.steps,features=features))
            k = 1
            is_train = False

        else:
            xs, ys = self.sess.run(self.dataset.get_all_dataset(),features=features)
            k = 1
            is_train = False

        return {self.X: xs, self.Y: ys, self.keep_prob: k, self.is_training : is_train}




    #TODO change train_iterator to iterator (confusing)
    def feed_dict_v3(self,train_iterator, train,epoch = 1, all_test=False):


        if train:
            xs, ys = self.sess.run(train_iterator)


            k = self.k_p_dropout
            is_train = True




            if (epoch>3000):
                learning_rate_decay =  (1/(1+10*epoch)) * self.learning_rate


            if (epoch>4000):
                learning_rate_decay =  (1/(1+100*epoch)) * self.learning_rate

            if (epoch>5000):
                learning_rate_decay =  (1/(1+200*epoch)) * self.learning_rate

            if (epoch>6000):
                learning_rate_decay =  (1/(1+1000*epoch)) * self.learning_rate

            if (epoch>7000):
                learning_rate_decay =  (1/(1+10000*epoch)) * self.learning_rate

            if (epoch > 8000):
                learning_rate_decay = (1 / (1 + 15000 * epoch)) * self.learning_rate

            if (epoch > 9000):
                learning_rate_decay = (1 / (1 + 200000 * epoch)) * self.learning_rate


            else:
                learning_rate_decay = learning_rate_decay =  (1/(1+0.01*epoch)) * self.learning_rate

                # print(learning_rate_decay)


        elif(not all_test):
            xs, ys = self.sess.run(train_iterator)
            k = 1
            is_train= False
            learning_rate_decay =  self.learning_rate

        else:
            xs, ys = self.sess.run(self.dataset.get_all_dataset())
            k = 1
            is_train = False
            learning_rate_decay =  self.learning_rate

        return {self.X: xs, self.Y: ys, self.keep_prob: k, self.is_training : is_train, self.learning_rate_dec : learning_rate_decay }



    def train(self,features=c.FEATURES[2:len(c.FEATURES)]):

        with tf.Session() as self.sess:
            feature_i = 3
            train_iterator = self.dataset.get_train_dataset(batch_size=self.batch_size, num_epochs=self.steps,features=c.FEATURES[feature_i:]) # c.feature_roman.FEATURES[3:] #features=c.FEATURES[2:len(c.FEATURES)])
            test_iterator = self.dataset.get_test_dataset(num_epochs=self.steps,features=c.FEATURES[feature_i:]) #c.feature_roman  #features=c.FEATURES[2:len(c.FEATURES)])
            # for i in range(10):
            #     print(sess.run(train_iterator))

            # train_iterator_handle = sess.run(train_iterator.string_handle())

            # test_iterator = self.dataset.get_test_dataset(num_epochs=self.steps)
            # test_iterator_handle = sess.run(test_iterator.string_handle())



            layers = {}                # activation_layer_1 = self.nn_layer(self.X, self.n_layer[0], self.n_layer[1], "layer_1", activation_fn= tf.nn.relu)
                                       # hypothesis = self.nn_layer(activation_layer_1, self.n_layer[1], self.n_layer[2], "layer_2", activation_fn= tf.identity)
            weights = []
            for index,l_ in enumerate(self.n_layer):

                if(index == len(self.n_layer)-2):
                    weight, bias, activation = self.nn_layer(layers[index]["activation"], self.n_layer[index], self.n_layer[index+1], "layer_"+str(index+1), layer_BN=False,activation_fn=tf.identity)
                    weights.append(weight)
                    layers.update({index+1 : {"weight":weight, "bias":bias ,"activation" : activation }})
                    break
                if (index == 0):
                     weight, bias, activation = self.nn_layer(self.X, self.n_layer[index], self.n_layer[index+1], "layer_"+str(index+1),layer_BN=True,activation_fn=self.activation_fn)
                     weights.append(weight)
                     layers.update({index+1 : {"weight": weight, "bias": bias, "activation" : activation }})
                else :
                    weight, bias, activation = self.nn_layer(layers[index]["activation"], self.n_layer[index], self.n_layer[index + 1],"layer_" + str(index + 1),layer_BN=True, activation_fn=self.activation_fn)
                    weights.append(weight)
                    layers.update({index + 1: {"weight": weight, "bias": bias, "activation" : activation}})



            _hypothesis = layers[len(self.n_layer)-1]["activation"]
            _accuracy=self.accuracy(_hypothesis)
            # rain_step, cost, self.Y, hypothesis
            _train_step,_loss, _Y, ___hypothesis = self.loss_and_train(_hypothesis,weights)
            tf.global_variables_initializer().run(session=self.sess)

            merged = tf.summary.merge_all()
            if self.board_metrics:
                train_writer = tf.summary.FileWriter(self.logdir + '/train', self.sess.graph)
                test_writer = tf.summary.FileWriter(self.logdir + '/test', self.sess.graph)



            for i in range(self.steps):
                if i %  10 ==0:
                    # summary, acc = sess.run([merged,_accuracy],feed_dict=self.feed_dict(False))
                    # train_writer.add_summary(summary, i)
                    # summary, acc,__hypo = sess.run([merged,_accuracy,_hypothesis], feed_dict=self.feed_dict_v2(False,features= features))
                    summary, acc, __hypo = self.sess.run([merged, _accuracy, _hypothesis],feed_dict=self.feed_dict_v3(test_iterator, False))
                    if self. board_metrics: test_writer.add_summary(summary,i)

                    print('num_epochs n°',i, 'accuracy', acc)
                    # if i %  20 ==0:
                    #     print("Y",__Y.shape,"HYPO",__hypo.shape)
                # elif (self.board_metrics and  i% 10001==0):
                #     # if i% 10000==0:
                #     run_options= tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)
                #     run_metadata = tf.RunMetadata()
                #     # summary, _,__loss = sess.run([merged,_train_step,_loss], feed_dict=self.feed_dict(True), options= run_options, run_metadata= run_metadata)
                #     # summary, _, __loss, ___Y,____hypothesis = sess.run([merged, _train_step, _loss, _Y, ___hypothesis], feed_dict=self.feed_dict_v2(True,features= features),options=run_options, run_metadata=run_metadata)
                #     summary, _, __loss, ___Y, ____hypothesis = sess.run([merged, _train_step, _loss, _Y, ___hypothesis],feed_dict=self.feed_dict_v3(test_iterator, True),options=run_options, run_metadata=run_metadata)
                #     # print(___Y.shape, ____hypothesis.shape)
                #     # _ = self.sess.run([__train_ste], feed_dict=self.feed_dict(False),options = run_options, run_metadata = run_metadata)
                #     train_writer.add_run_metadata(run_metadata,'num_epochs_%04d' % i )
                #     train_writer.add_summary(summary,i)
                #     print("num_epochs n°:", i, "  LOSS:", np.sqrt(__loss))
                else:
                        # for i in range(250):
                    # summary, _, __loss = sess.run([merged, _train_step, _loss],feed_dict=self.feed_dict_v2(True,features= features))
                    summary,_, __loss = self.sess.run([merged,_train_step, _loss], feed_dict=self.feed_dict_v3(train_iterator, True,i))
                    if self.board_metrics: train_writer.add_summary(summary, i)

                    #_, __loss= sess.run([_train_step, _loss],feed_dict=self.feed_dict_v2(train_iterator, True))
                    if i % 9== 0:
                            print("num_epochs n°:", i, "  LOSS:", np.sqrt(__loss))

                        # summary, _ = sess.run([merged, _train_step, ], feed_dict=self.feed_dict( True))


            #---------------------------------------Delete----------------------------------------->



            MFLT_F_dataset_original = pd.read_csv(
                "/home/ms255613/BHAG/BHAG/TESTS/Automatic_tests/ANN/input_data/test2.csv", sep=";",
                header=2)

            pd.options.display.float_format = '{:.4f}'.format
            TMFLT_F_dataset = MFLT_F_dataset_original
            dataset_features_normalized = fm.preprocess_features(TMFLT_F_dataset)
            dataset_features_normalized_vdd = fm.preprocess_features(TMFLT_F_dataset)
            dataset_features_normalized = nf.normalize_x_scale(dataset_features_normalized)
            dataset_dictionary_ordained_by_die = {}
            dataset_dictionary_ordained_by_die_vdd = {}
            dataset_dictionary_ordained_by_die_prediction = {}
            dataset_dictionary_ordained_by_die.update(
                {die: dataset_features_normalized.loc[dataset_features_normalized['Die'] == int(die)]
                 for die in c.DIES})
            dataset_dictionary_ordained_by_die_vdd.update(
                {die: dataset_features_normalized_vdd.loc[
                    dataset_features_normalized_vdd['Die'] == int(die)] for die in c.DIES})

            for die in c.DIES:
                Vdd = np.array(dataset_dictionary_ordained_by_die_vdd[die]["Vdd"])
                features_test = dataset_dictionary_ordained_by_die[die][
                    c.FEATURES[feature_i:len(c.FEATURES)]]  # c.FEATURES[2:len(c.FEATURES)]
                target_test = dataset_dictionary_ordained_by_die[die][c.TARGET]
                fmax = np.array(dataset_dictionary_ordained_by_die[die][c.TARGET])

                test_iterator = ib.my_input_fn_v2(features_test,target_test,
                                                                       num_epochs=None,
                                                                       batch_size=target_test.shape[0],
                                                                       shuffle=False)

                by_die_predictions = self.sess.run(_hypothesis,feed_dict=self.feed_dict_v3(test_iterator, False))


                # input_test_by_die = ib.my_input_fn_v2(features_test, target_test["Fmax"], num_epochs=1,
                #                                       shuffle=False,
                #                                       batch_size=target_test.shape[0])
                # # print("Die ", die, " target_test",target_test)
                # by_die_predictions = model.predict(input_fn=input_test_by_die, path=path, pruning=False,
                #                                    treshold=41, treshold_show=0.005)

                # print(type(by_die_predictions))
                tmp_p = by_die_predictions
                Error = np.subtract(fmax, tmp_p)
                Error = np.divide(Error, fmax) * 100

                # print(type(Error),Error.shape)
                dataset_dictionary_ordained_by_die_prediction.update(
                    {die: {"Vdd": Vdd, "Error": Error}})

            dv.plot_error_curves_by_die(dataset_dictionary_ordained_by_die_prediction)
            dataset_dictionary_error_by_vdd = {}
            treatments = []
            fig, ax = plt.subplots()
            ax.set_axisbelow(True)
            a = [str(i) for i in list(np.sort(dataset_features_normalized.Vdd.unique()))]
            # plt.show()
            ax.set_title('Box plot of the Error')
            ax.set_xlabel('Voltage (V)')
            ax.set_ylabel('Error (%)')

            for vdd in np.sort(dataset_features_normalized.Vdd.unique()):
                data = dataset_features_normalized.loc[dataset_features_normalized['Vdd'] == int(vdd)]

                X = data[c.FEATURES[feature_i:len(c.FEATURES)]]
                # fmax = np.array(data[c.TARGET])
                fmax = data[c.TARGET]

                # input_test_by_die = ib.my_input_fn_v2(X, fmax, num_epochs=1, shuffle=False,
                #                                       batch_size=target_test.shape[0])
                #
                # by_die_predictions = model.predict(input_fn=input_test_by_die, path=path, pruning=False, treshold=41,
                #                                    treshold_show=0.005)

                test_iterator = ib.my_input_fn_v2(X, fmax,
                                                  num_epochs=None,
                                                  batch_size=X.shape[0],
                                                  shuffle=False)

                by_die_predictions = self.sess.run(_hypothesis, feed_dict=self.feed_dict_v3(test_iterator, False))



                tmp_p = by_die_predictions
                # fmax = np.concatenate(fmax, axis=0)

                tmp_p = tmp_p.reshape(tmp_p.shape[0], 1)
                # print(fmax.shape,tmp_p.shape)
                # print(type(fmax),type(tmp_p))
                Error = np.subtract(fmax, tmp_p)

                Error = np.abs(np.divide(Error, fmax) * 100)

                dataset_dictionary_error_by_vdd.update({vdd: Error})
                treatments.append(Error)

            pos = np.array(range(len(treatments))) + 5
            bp = ax.boxplot(treatments, sym='k+')
            xtickNames = plt.setp(ax, xticklabels=np.repeat(a, 2))
            plt.setp(xtickNames, rotation=75, fontsize=8)

            plt.show()

            dataset_features_normalized = fm.preprocess_features(TMFLT_F_dataset)
            dataset_features_normalized_vdd = fm.preprocess_features(TMFLT_F_dataset)
            dataset_features_normalized = nf.normalize_x_scale(dataset_features_normalized)

            data = dataset_features_normalized

            X = data[c.FEATURES[feature_i:len(c.FEATURES)]]
            # fmax = np.array(data[c.TARGET])
            fmax = data[c.TARGET]


            test_iterator = ib.my_input_fn_v2(X, fmax,
                                              num_epochs=None,
                                              batch_size=X.shape[0],
                                              shuffle=False)

            validation_predictions= self.sess.run(_hypothesis, feed_dict=self.feed_dict_v3(test_iterator, False))

            validation_predictions = validation_predictions.reshape(validation_predictions.shape[0], 1)

            mse = mean_squared_error(validation_predictions, fmax)
            print("Total MSE: %.4f" % mse)

            r2 = metrics.r2_score(fmax, validation_predictions)
            print("Total r2: %.4f" % r2)

            # mse = mean_squared_error(y_test.values, clf.predict(X_test.values))

            # print(validation_targets)
            # print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
            # print(validation_predictions)
            # print("validation_targets", type(validation_targets), "validation_predictions", type(validation_predictions))

            rse = np.sqrt(((fmax.values - validation_predictions) ** 2).sum()) / (13 - 2)
            sst = ((fmax.values - np.average(fmax.values)) ** 2).sum()
            sse = ((fmax.values - validation_predictions) ** 2).sum()
            r2 = metrics.r2_score(fmax.values, validation_predictions)

            # acc_train =metrics.accuracy_score(training_targets["Fmax"].values, training_predictions)
            # print("Accurracy training", acc_train)
            # acc_validation= metrics.accuracy_score(validation_predictions, validation_targets["Fmax"].values)
            # print("Accurracy validation", acc_validation)


            print("Total MSE: %.4f" % mse)

            print("Total rse: %.4f" % rse)
            print("Total sst: %.4f" % sst)
            print("Total sse: %.4f" % sse)
            print("Total R2: %.4f" % r2)










            #---------------------------------------------------------------------------------------->

                            #

            if self.board_metrics:
                self.f.write("Loss :" + str(np.sqrt(__loss))+"\n")
                self.f.write("Accuracy :" + str(acc)+ "\n")
                self.f.close()

                outdir = self.logdir + 'Weights_Bias'
                if not os.path.exists(outdir):
                    os.mkdir(outdir)

                with self.sess.as_default():
                    print("ENTRE")

                    # variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
                    # values = {}
                    # print(variables)
                    # for variable in variables:
                    #     values[variable.name[:-2]] = self.sess.run([variable],
                    #                                                feed_dict=self.feed_dict_v3(test_iterator,
                    #                                                                            False))
                    #
                    #     print(values[variable.name[:-2]])



                values = {}
                for index, value in enumerate(self.n_layer):
                    if index == len(self.n_layer)-1:
                        break
                    #
                    # if (False):
                    #     moving_mean= values["batch_normalization/moving_mean"]
                    #     moving_variance= values["batch_normalization/moving_variance"]
                    #     gamma = values["batch_normalization/gamma"]
                    #     beta = values["batch_normalization/beta"]
                    #
                    #
                    #     print(moving_mean)
                    #     print(moving_variance)
                    #     print(gamma)
                    #     print(beta)


                    #     moving_mean = pd.DataFrame(data=moving_mean.astype(float))
                    #     moving_variance = pd.DataFrame(data=moving_variance.astype(float))
                    #     gamma = pd.DataFrame(data=gamma.astype(float))
                    #     beta = pd.DataFrame(data=beta.astype(float))
                    #
                    #     moving_mean.to_csv(os.path.join(outdir, 'moving_mean' + str(index + 1) + '.csv'), sep=';', header=False,
                    #              float_format='%.4f', index=False)
                    #     moving_variance.to_csv(os.path.join(outdir, 'moving_variance' + str(index + 1) + '.csv'), sep=';', header=False,
                    #              float_format='%.4f', index=False)
                    #     gamma.to_csv(os.path.join(outdir, 'gamma' + str(index + 1) + '.csv'), sep=';', header=False,
                    #              float_format='%.4f', index=False)
                    #     beta.to_csv(os.path.join(outdir, 'beta' + str(index + 1) + '.csv'), sep=';', header=False,
                    #              float_format='%.4f', index=False)
                    #
                    weight = layers[index + 1]["weight"]
                    bias = layers[index + 1]["bias"]
                    __weight, __bias = self.sess.run([weight, bias], feed_dict=self.feed_dict_v3(test_iterator, False))
                    # __weight, __bias = sess.run([weight, bias], feed_dict=self.feed_dict( False))
                    w = pd.DataFrame(data=__weight.astype(float))
                    w.to_csv(os.path.join(outdir, 'w' + str(index + 1) + '.csv'), sep=';', header=False,
                             float_format='%.4f', index=False)
                    b = pd.DataFrame(data=__bias.astype(float))
                    b.to_csv(os.path.join(outdir, 'b' + str(index + 1) + '.csv'), sep=';', header=False,
                             float_format='%.4f', index=False)

            if self.board_metrics:
                train_writer.close()
                test_writer.close()
                print("tensorboard --logdir=%s --port=  " % self.logdir)

        #
            # with self.sess.as_default():
            #     print("ENTRE")
            #
            #     variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
            #     values = {}
            #     for variable in variables:
            #         values[variable.name[:-2]] =  self.sess.run([variable],feed_dict=self.feed_dict_v3(test_iterator, False))
            #         print(variable)
            #     print("################################################")
            #     print('moving mean layer 1 is:')
            #     print(values["batch_normalization/moving_mean"])
            #     print("################################################")
            #     print('moving mean layer 1 is:')
            #     print(values["batch_normalization/moving_variance"])
            #
            #     print("################################################")
            #     print('gamma 1:')
            #     print(values["batch_normalization/gamma/Adam"])
            #     print(values["batch_normalization/gamma/Adam_1"])
            #     print(values["batch_normalization/gamma"])
            #     print("################################################")
            #     print('beta 1:')
            #     print(values["batch_normalization/beta/Adam"])
            #     print(values["batch_normalization/beta/Adam_1"])
            #     print(values["batch_normalization/beta"])
            #     print("################################################")

        # <tf.Variable 'batch_normalization_2/beta/Adam_1:0' shape=(36,) dtype=float64_ref>
        self.sess.close()
        return acc ,np.sqrt(__loss)

    def test(self,dataset,path): # use W and B from train



        self.sess = tf.InteractiveSession()
        sess =self.sess
        self.dataset = dataset

        test_iterator = self.dataset.get_test_dataset(num_epochs=1,features=c.FEATURES[2:len(c.FEATURES)])#features=c.FEATURES[2:len(c.FEATURES)]
        # test_iterator = self.dataset.get_all_dataset(c.feature_roman)
        # train_iterator = self.dataset.get_train_dataset(batch_size=self.batch_size, num_epochs=self.steps,
        #                                                 features=c.feature_roman)
        # test_iterator = self.dataset.get_train_dataset(batch_size=self.dataset.training_examples.shape[0], num_epochs=1,features=c.feature_roman)

        layers = os.listdir(path)




        activations = []
        for i in range(int(len(layers) / 2)):  # n° of layers = (#B+#W)/2
            b = pd.read_csv(path + "/b" + str(i + 1) + ".csv", sep=";", header=None)
            w = pd.read_csv(path + "/w" + str(i + 1) + ".csv", sep=";", header=None)

            w_tf = tf.Variable(name="w" + str(i + 1), initial_value=w)
            b_tf = tf.Variable(name="b" + str(i + 1), initial_value=b)
            if i == 0:
                pre_activation = tf.add(tf.matmul(self.X, w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))
            elif i == len(layers) - 1:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(tf.identity(pre_activation))
            else:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))

        # all_iterator = self.dataset.get_all_dataset()


        tf.global_variables_initializer().run()


        if (self.model == "Lin_Reg"):


            # accur = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction, self.Y)), 100)  # Error prediction in %
            # accur = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction, self.Y)), 100)  # Error prediction in %


            correct_prediction = tf.subtract(self.Y, activations[-1])
            # accuracy = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction,self.Y)), 100) # Error prediction in %

            # a = tf.reduce_mean(tf.equal(self.Y,hypothesis))
            accur = 100 - tf.abs(tf.reduce_mean(correct_prediction))


            # _acc, _Y,_h = sess.run([accur,self.Y,activations[-1]], self.feed_dict_v2(train=False,all_test=False))
            _acc, _Y, _h =sess.run([accur,self.Y,activations[-1]], feed_dict=self.feed_dict_v3(test_iterator, False))

            major_ticks = np.arange(min(_Y), max(_Y), 50)
            minor_ticks = np.arange((min(_Y)), max(_Y), 150)
            plt.xticks(minor_ticks)
            plt.yticks(major_ticks)
            plt.grid(which='minor', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)
            plt.grid(which='major', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)

            plt.title(' Fmax  Prediction Vs Measured ')
            plt.xlabel('Fmax  Measured')
            plt.ylabel('Fmax Predicted')

            plt.scatter(_Y, _h, marker='^')

            plt.plot([_h.min(), _h.max()],
                     [_h.min(), _h.max()], 'k--', lw=3)
            plt.legend()
            # And a corresponding grid

            # Or if you want different settings for the grids:
            plt.grid(which='minor', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)
            plt.grid(which='major', alpha=0.2, color='r', linestyle='-.', linewidth=0.5)
            plt.show()
            print('mean accuracy = ',100-abs(_acc))


        if( self.model == "Log_Reg"):
            prediction = tf.equal(self.Y, tf.argmax(activations[-1], 1))
            prediction_cast = tf.cast(prediction, tf.float32)
            accur = tf.reduce_mean(prediction_cast)

            print(sess.run([accur], self.feed_dict_v2(train=False,all_test=False)))

            print(self.sess.run([accur],feed_dict=self.feed_dict_v3(test_iterator,False)))




    def predict(self, path,input_fn,pruning,treshold,treshold_show):



        self.sess = tf.InteractiveSession()
        sess =self.sess
        self.dataset = input_fn

        layers = os.listdir(path)


        activations = []
        for i in range(int(len(layers) / 2)):  # n° of layers = (#B+#W)/2
            b = pd.read_csv(path + "/b" + str(i + 1) + ".csv", sep=";", header=None)
            w = pd.read_csv(path + "/w" + str(i + 1) + ".csv", sep=";", header=None)

            # if i ==2:
            #Prunning
            if pruning:
                print("W : ", i + 1, "  ", w[np.abs(w) < treshold_show].count().sum())
                w[np.abs(w) < treshold] = 0
                outdir = path + 'sparse'
                if not os.path.exists(outdir):
                    os.mkdir(outdir)

                w.to_csv(os.path.join(outdir, 'w' + str(i + 1) + '.csv'), sep=';', header=False, float_format='%.4f',
                         index=False)




            w_tf = tf.Variable(name="w" + str(i + 1), initial_value=w)
            b_tf = tf.Variable(name="b" + str(i + 1), initial_value=b)
            if i == 0:
                pre_activation = tf.add(tf.matmul(self.X, w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))
            elif i == len(layers) - 1:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(tf.identity(pre_activation))
            else:
                pre_activation = tf.add(tf.matmul(activations[-1], w_tf), b_tf)
                activations.append(self.activation_fn(pre_activation))

        # all_iterator = self.dataset.get_all_dataset()


        tf.global_variables_initializer().run()


        if (self.model == "Lin_Reg"):

            correct_prediction  = tf.subtract(self.Y, activations[-1])
            accur = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction, self.Y)), 100)  # Error prediction in %
            X, Y = sess.run(self.dataset)
            _acc, _Y,_h = sess.run([accur,self.Y,activations[-1]],{self.X : X , self.Y: Y, self.keep_prob: 1})
            print("accuracy", _acc)
            return _h


        if( self.model == "Log_Reg"):
            prediction = tf.equal(self.Y, tf.argmax(activations[-1], 1))
            prediction_cast = tf.cast(prediction, tf.float32)
            accur = tf.reduce_mean(prediction_cast)

            return prediction



#TODO ################################## GET VARIABLES #################################



# with sess.as_default():
#     def get_variables_values():
#         variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
#         values = {}
#         for variable in variables:
#             values[variable.name[:-2]] = sess.run(variable, feed_dict={
#                 x:batch[0], y_:batch[1], is_training:True
#                 })
#         return values
#
#
#     for i in range(t_iter):
#         batch = mnist.train.next_batch(batch_size)
#         if i%100 == 0: #test-set summary
#             print('####################################')
#             values = get_variables_values()
#             print('moving variance is:')
#             print(values["conv1_bn/moving_variance"])
#             print('moving mean is:')
#             print(values["conv1_bn/moving_mean"])
#             print('gamma is:')
#             print(values["conv1_bn/gamma/Adam"])
#             print('beta is:')
#             print(values["conv1_bn/beta/Adam"])
#             summary, acc = sess.run([merged,accuracy], feed_dict={
#                 x:mnist.test.images, y_:mnist.test.labels, is_training:False


#BN https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top


# <tf.Variable 'W_layer_1:0' shape=(12, 36) dtype=float64_ref>
# <tf.Variable 'layer_1/bias/bias:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'batch_normalization/gamma:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization/beta:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization/moving_mean:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization/moving_variance:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'W_layer_2:0' shape=(36, 36) dtype=float64_ref>
# <tf.Variable 'layer_2/bias/bias:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'batch_normalization_1/gamma:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_1/beta:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_1/moving_mean:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_1/moving_variance:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'W_layer_3:0' shape=(36, 36) dtype=float64_ref>
# <tf.Variable 'layer_3/bias/bias:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'batch_normalization_2/gamma:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_2/beta:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_2/moving_mean:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_2/moving_variance:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'W_layer_4:0' shape=(36, 1) dtype=float64_ref>
# <tf.Variable 'layer_4/bias/bias:0' shape=(1, 1) dtype=float32_ref>
# <tf.Variable 'train/beta1_power:0' shape=() dtype=float32_ref>
# <tf.Variable 'train/beta2_power:0' shape=() dtype=float32_ref>
# <tf.Variable 'W_layer_1/Adam:0' shape=(12, 36) dtype=float64_ref>
# <tf.Variable 'W_layer_1/Adam_1:0' shape=(12, 36) dtype=float64_ref>
# <tf.Variable 'layer_1/bias/bias/Adam:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'layer_1/bias/bias/Adam_1:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'batch_normalization/gamma/Adam:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization/gamma/Adam_1:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization/beta/Adam:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization/beta/Adam_1:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'W_layer_2/Adam:0' shape=(36, 36) dtype=float64_ref>
# <tf.Variable 'W_layer_2/Adam_1:0' shape=(36, 36) dtype=float64_ref>
# <tf.Variable 'layer_2/bias/bias/Adam:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'layer_2/bias/bias/Adam_1:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'batch_normalization_1/gamma/Adam:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_1/gamma/Adam_1:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_1/beta/Adam:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_1/beta/Adam_1:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'W_layer_3/Adam:0' shape=(36, 36) dtype=float64_ref>
# <tf.Variable 'W_layer_3/Adam_1:0' shape=(36, 36) dtype=float64_ref>
# <tf.Variable 'layer_3/bias/bias/Adam:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'layer_3/bias/bias/Adam_1:0' shape=(1, 36) dtype=float32_ref>
# <tf.Variable 'batch_normalization_2/gamma/Adam:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_2/gamma/Adam_1:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_2/beta/Adam:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'batch_normalization_2/beta/Adam_1:0' shape=(36,) dtype=float64_ref>
# <tf.Variable 'W_layer_4/Adam:0' shape=(36, 1) dtype=float64_ref>
# <tf.Variable 'W_layer_4/Adam_1:0' shape=(36, 1) dtype=float64_ref>
# <tf.Variable 'layer_4/bias/bias/Adam:0' shape=(1, 1) dtype=float32_ref>
# <tf.Variable 'layer_4/bias/bias/Adam_1:0' shape=(1, 1) dtype=float32_ref>
