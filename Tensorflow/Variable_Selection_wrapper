import tensorflow as tf
import numpy as np
from datetime import datetime
import pandas as pd
import os
#
from project.Testing_things.tmfl_data import Tmfl_datset
# from project.boosting_regression import by_die_predictions
from project.generic_functions import get_logdir
from project.Testing_things.create_nn_model import *
from  tensorflow.examples.tutorials.mnist import input_data

from project import input_builder as ib
from project import constants  as c
from project import features_manager as fm
from project import normalize_functions as nf
from project import Regressor as R
from project import data_visualization as dv
import pandas as pd

acc2 = 90
acc1 = 40

best1= 0
best2= 1000
accurate = {}

features_not_used_list = c.FEATURES[2:len(c.FEATURES)]
features_already_used_list = []


# dataset = Tmfl_datset()
# dataset.input_data(path_file="/ANN/input_data/test2.csv")
sensors = []
num_epochs = 30000  # n° repetition
batch_size = 256  # n° input entries
iteration = 10
model = "Lin_Reg"
activation_fn = tf.nn.elu
learning_rate = 0.001

def feed_dict_v3(train_iterator, train, all_test=False):
    if train:
        xs, ys = sess.run(train_iterator)
        k = 1

    elif (not all_test):
        xs, ys =sess.run(train_iterator)
        k = 1

    else:
        xs, ys = sess.run(dataset.get_all_dataset())
        k = 1

    return {X: xs, Y: ys }


sensor = np.random.randint(2, len(c.FEATURES))
sensors.append(c.FEATURES[sensor])

taken = True

for l in range (9999999):

    accurate = {}



    for feature_i in features_not_used_list:

        # print("TAKEN ------->  1", taken)
        print("features_already_used_list ----> ", features_already_used_list)
        if taken:

            accuracy_per_feature = np.array([])
            sensors=[]
            sensors.append(feature_i)
            sensors += features_already_used_list
            n_layer = [len(sensors), 26, 26, 26, 1]

        else :

            sensors.clear()
            sensors = features_already_used_list
            n_layer = [len(sensors), 26, 26, 26, 1]
        print("features_already_used_list ----> ", features_already_used_list)
        print("SENSORS ----> ", sensors)


        for j in range(iteration):

            # print("TAKEN -------> 2", taken)
            print("#################################___LOSS___######################################\n")
            print("Iteration", j+1, "/",iteration)
            dataset = Tmfl_datset()
            dataset.input_data(path_file="/home/ms255613/BHAG/BHAG/TESTS/Automatic_tests/ANN/input_data/test2.csv")
            print("#################################___LOSS___######################################\n")
            tf.reset_default_graph()

            X = tf.placeholder(tf.float32, shape=(None, n_layer[0]), name='X')
            Y = tf.placeholder(tf.float32, shape=(None), name='Y')

            # sess = tf.Session()
            # sess = tf.Session(config=tf.ConfigProto(
            #     intra_op_parallelism_threads=8))


            # results = [int(i) for i in n_layer]

            # X = tf.placeholder(tf.float32, shape=(None, n_layer[0]), name='X')
            # Y = tf.placeholder(tf.float32, shape=(None), name='Y')
            #

            # First layer of weights

            W1 = tf.get_variable("W1", shape=[n_layer[0],n_layer[1]],
                                 initializer=tf.glorot_uniform_initializer(), dtype=tf.float32)
            b1= tf.Variable(tf.zeros(shape=(1, n_layer[1])), name="b1")
            # b1 = tf.cast(b1, tf.float32)
            layer1= tf.add(tf.matmul(X,W1 ),b1)
            layer1_act = activation_fn(layer1)

            # Second layer of weights

            W2 = tf.get_variable("W2", shape=[n_layer[1], n_layer[2]],
                                 initializer=tf.glorot_uniform_initializer(), dtype=tf.float32)
            b2= tf.Variable(tf.zeros(shape=(1, n_layer[2])), name="b2")
            b2 = tf.cast(b2, tf.float32)
            layer2= tf.add(tf.matmul(layer1,W2 ),b2)
            layer2_act = activation_fn(layer2)

            # Third layer of weights

            W3 = tf.get_variable("W3", shape=[n_layer[2], n_layer[3]],
                                 initializer=tf.glorot_uniform_initializer(), dtype=tf.float32)
            b3= tf.Variable(tf.zeros(shape=(1, n_layer[3])), name="b3")
            # b3 = tf.cast(b3, tf.float32)
            layer3= tf.add(tf.matmul(layer2,W3 ),b3)
            layer3_act = activation_fn(layer3)

            # Fourth layer of weights

            W4 = tf.get_variable("W4", shape=[n_layer[3], n_layer[1]],
                                 initializer=tf.glorot_uniform_initializer(), dtype=tf.float32)
            b4= tf.Variable(tf.zeros(shape=(1, n_layer[4])), name="b4")
            # b4 = tf.cast(b4, tf.float32)
            layer4= tf.add(tf.matmul(layer2,W3 ),b3)

            loss = tf.reduce_mean(tf.square(layer4-Y))

            grads_and_vars = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9,beta2=0.899).compute_gradients(loss, [b1,W1])

            train_step=tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.899).minimize(loss)

            # train_step = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.899).

            # train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)
            #Accuracy
            correct_prediction = tf.reduce_mean(tf.square(layer4-Y))
            accuracy2 = correct_prediction
            # accuracy = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction, Y)), 100)  # Error prediction in %
            # accuracy2 =  100 - np.abs(accuracy)


            # We need to define the parts of the network needed for learning a policy
            # tf.global_variables_initializer().run(session=sess)

            with tf.Session() as sess:
                sess.run(tf.global_variables_initializer())
                train_iterator = dataset.get_train_dataset(batch_size=batch_size, num_epochs=num_epochs,features=sensors)
                test_iterator = dataset.get_test_dataset(num_epochs=num_epochs,features=sensors)

                for i in range(num_epochs):
                    # if i % 10 == 0 :print(i)

                    # xs, ys = sess.run(dataset.get_train_dataset(batch_size=batch_size, num_epochs=num_epochs, features=sensors))



                    # _= sess.run([train_step], feed_dict={X: xs,Y: ys})
                    # _,grads_and_vars1 = sess.run([train_step,grads_and_vars], feed_dict=feed_dict_v3(train_iterator, True))
                    # grads_and_vars2 = sess.run([grads_and_vars], feed_dict=feed_dict_v3(train_iterator, True))
                    _,acc1 = sess.run([train_step,loss], feed_dict=feed_dict_v3(train_iterator, True))

                    # _, grads_and_vars2= sess.run([train_step, grads_and_vars],  feed_dict=feed_dict_v3(train_iterator, True))
                    # print("grads_and_vars  ",grads_and_vars2)
                    # summary, _, __loss = sess.run([merged, _train_step, _loss],feed_dict=self.feed_dict_v3(train_iterator, True))


                        # xs, ys = sess.run(dataset.get_test_dataset(num_epochs=num_epochs, features=sensors))
                        # acc1, hypo = sess.run([accuracy2, layer4], feed_dict={X: xs, Y: ys})
                    # acc1 = sess.run([accuracy2], feed_dict=feed_dict_v3(test_iterator, False))
                    # acc1 = sess.run([loss], feed_dict=feed_dict_v3(test_iterator, False))
                    # _,acc1= sess.run([train_step,loss], feed_dict=feed_dict_v3(train_iterator, True))

                    # loss
                    # print(acc)
                    # if i % 100 == 0: print("Sensors", sensors, "epoch_N°: _", i, " Loss : ", np.sqrt(acc1))
                    # if i % 10000 == 0: print("Sensors", sensors, "epoch_N°: _",i," accuracy : ",acc1 )
                    if i % 10000 == 0: print("Sensors", sensors, "epoch_N°: _", i, " Error : ", acc1)
                    # if i % 10000 == 0: print("Sensors", sensors, "epoch_N°: _", i, " Loss : ", np.sqrt(acc1))

            print("TAKEN -------> 3", taken)

            accuracy_per_feature=np.append(accuracy_per_feature, np.sqrt(acc1 ))
            accurate.update({feature_i: np.mean(accuracy_per_feature)})
            sess.close()
            # print("Sensors ->",sensors," iteration ", j, "/",iteration,"accuracy",accuracy_per_feature,"\r")
            # print("Sensors ->", sensors, " iteration ", j, "/", iteration, "Loss_per_feature", accuracy_per_feature, "\r")
            # print("Feature", feature_i,"iteration : ", j, "/", iteration, " accuracy_per_feature", accuracy_per_feature, "\r")
            print("Feature", feature_i, "iteration : ", j, "/", iteration, " Error_per_feature",
                  accuracy_per_feature, "\r")


            # print("accurate_dict", accurate, "\r")
            print("error_dict", accurate, "\r")
            # print("Loss_dict", accurate, "\r")

        if not taken:

            best3 = best2
            best2 = np.mean(accuracy_per_feature)

            print("No taken, refresh Best2 from ",best3, "to ----> ",best2 )

            break
        else:
            best_feature = min(accurate.items(), key=operator.itemgetter(1))[0]
            best1 = min(accurate.items(), key=operator.itemgetter(1))[1]

    # print("TAKEN -------> 4", taken)

    if taken:
        print("best1", best1, "best2", best2, "\r")
        if best1<=best2 :

            # print("TAKEN -------> QUE", taken)
            best2=best1
            # best_feature = max(accurate.items(), key=operator.itemgetter(1))[1]
            print("best_feature",best_feature)
            features_already_used_list.append(best_feature)
            features_not_used_list.remove(best_feature)
            print("features_not_used_list", features_not_used_list, "\r")
            print("features_already_used_list", features_already_used_list, "\r")


        else:
            taken = False

    else:
        taken = True
    accurate.clear()


    print("features_not_used_list", features_not_used_list, "\r")
    print("features_already_used_list", features_already_used_list, "\r")
    print("TAKEN -------> 5", taken)





                # acc1=np.abs(acc1)
                #
                #
                # sess.close()
                #
                # print("acc1",acc1,"acc2",acc2)
                #
                # if (acc1 > acc2):
                #
                #     print("DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDdd")
                #     print ("entre" , "acc1  ",  acc1,"acc2  ",  acc2,)
                #     acc2 = acc1
                #     while True:
                #         sensor = np.random.randint(2, len(c.FEATURES))
                #         if c.FEATURES[sensor] not in sensors:
                #
                #             break
                #     sensors.append(c.FEATURES[sensor])
                #
                # else:
                #     if(len(sensors)>0):
                #         sensors.pop()
                #     elif np.random.rand() > 0.9:
                #         sensors.pop(np.random.randint(0, len(sensors)))
                #
                #
                #     while True:
                #         sensor = np.random.randint(2, len(c.FEATURES))
                #         if c.FEATURES[sensor] not in sensors:
                #             break
                #     sensors.append(c.FEATURES[sensor])
                #


                # print(sensors)
            #
        #
        #
