import tensorflow as tf
import numpy as np
from datetime import datetime
import pandas as pd
import os
from project.generic_functions import  get_logdir

                     
class NN_model():

    Y = tf.placeholder(tf.float32, shape= (None), name='Y')
    keep_prob = tf.placeholder(tf.float32)

    def __init__(self, layers_n, model,learning_rate,logdir= "/ANN/log"):
            self.X = tf.placeholder(tf.float32,shape= (None,layers_n), name='X')
            self.model = model
            self.logdir = get_logdir(logdir)
            self.learning_rate = learning_rate

    def __init__2 (self):
            merged = tf.summary.merge_all()
            sess = tf.InteractiveSession()
            train_writer = tf.summary.FileWriter(self.logdir+'/train',sess.graph)
            test_writer = tf.summary.FileWriter(self.logdir+'test',sess.graph)
            tf.global_variables_initializer().run()                                  

    def variables_summaries(self,name,var):
        with tf.name_scope("summaries/"+name):
            mean = tf.reduce_mean(var)
            tf.summary.scalar("mean",mean)
            with tf.name_scope("stddev"):
                stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))  # stddev is used to quantify the amount of variation or dispersion of a set of data values
                                                                       # A low standard deviation indicates that the data points tend to be close to the mean
                                                                       # (also called the expected value) of the set, while a high standard deviation indicates
                                                                       # that the data points are spread out over a wider range of values.
                tf.summary.scalar(stddev)
                tf.summary.scalar("max",tf.reduce_mean(tf.argmax(stddev)))
                tf.summary.scalar("min",tf.reduce_mean(tf.argmin(stddev)))
            tf.summary.histogram(var)


    def nn_layer(self,input_tensor, input_dim, output_dim, layer_name, activation_fn= tf.nn.relu):

        assert input_tensor.shape[1]==input_dim, "Check dimensions"

        with tf.name_scope(layer_name):
            with tf.name_scope("weights")
                weight = tf.get_variable("W", shape=(input_dim,output_dim), initializer= tf.contrib.layers.xavier_initializer())
                self.variables_summaries("Weight",weight)
            with tf.name_scope("bias"):
                bias = tf.Variable(tf.zeros(shape=(1,output_dim)),name="bias")
                self.variables_summaries("Bias",bias)
            with tf.name_scope("pre_activation"):
                pre_activation = tf.add(tf.matmul(input_tensor*weight),bias)
                tf.summary.histogram("pre_activation",pre_activation)
            with tf.name_scope("activation"):
                activation= activation_fn(pre_activation)
                tf.summary.histogram("activation", activation)

    def droput(self,layer):
        with tf.name_scope("dropout"):
            tf.summary.scalar("k_droput",self.keep_prob)
            dropped = tf.nn.dropout(layer,self.keep_prob)
        return  dropped

    def loss_and_train (self, hypothesis):

        if (self.model == "Log_Reg"):
            with tf.name_scope("cross_entropy"):
                loss = tf.losses.sparse_softmax_cross_entropy(labels=self.Y,logits=hypothesis)

        if (self.model == "Lin_Reg"):
            with tf.name_scope("MSE"):
                loss = tf.losses.mean_squared_error(labels=self.Y,predictions=hypothesis)
        tf.summary.scalar("loss",loss)
        self.variables_summaries("loss",loss)
        train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)              


        


    def accuracy(self,hypothesis):
        with tf.name_scope("accuracy"):
            with tf.name_scope("correct_prediction"):
                if (self.model == "Log_Reg"):
                    y_test = tf.argmax(hypothesis,1)
                    correct_prediction = tf.equal(tf.argmax(hypothesis,1),self.Y)
                    accuracy =  tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

                if (self.model == "Lin_Reg"):
                    correct_prediction = tf.subtract(self.Y,hypothesis)
                    accuracy = tf.multiply(tf.reduce_mean(tf.truediv(correct_prediction,self.Y)),100) # Error prediction in %


            tf.summary.scalar("accuracy", accuracy)
            self.variables_summaries("accuracy",accuracy)








    def feed_train(self,train,dataset_train,dataset_test):

        if train:
            xs, ys = dataset_train
            k = 0.9
        else :
            xs, ys = dataset_test
            k = 1

        return {self.X : xs, self.Y:ys, self.keep_prob: k }





